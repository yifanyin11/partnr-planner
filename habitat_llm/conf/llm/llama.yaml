llm:
  _target_    : habitat_llm.llm.Llama
  _partial_   : True

# Put your host number here if you are using RLM
host          : ""
port          : 4449
serverdir     : ""

# The default of inference mode of llama3 is hf (hugging face)
# You can choose between "hf" and "rlm" (remote language model)
inference_mode          : hf

# System, User, Assistant and end of turn tags
system_tag : "<|start_header_id|>system<|end_header_id|>\n\n"
user_tag : "<|start_header_id|>user<|end_header_id|>\n\n"
assistant_tag : "<|start_header_id|>assistant<|end_header_id|>\n\n"
eot_tag : "<|eot_id|>"

generation_params:

  engine          : "{PATH to Meta Llama}/Meta-Llama-3.1-70B-Instruct"

  # The prompt to start the generation from.
  prompt        : ''

  # The maximum number of tokens to generate in the completion.
  max_tokens    : 250

  # float , optional, defaults to 1.0) â€” The value used to modulate the next token probabilities.
  temperature   : 0.0

  # Returns the best `n` out of `best_of` completions made on server side
  n             : 1
  best_of       : 1

  # up to 4 sequences that stop the generation
  stop          : '\n'

  # Return a list of response or not
  batch_response: False

  # If we want to do sample method of text generation or not
  do_sample     : False

  # The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.
  repetition_penalty : 1.0

  # The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.
  top_k         : 50

  # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation. Default to 1.0
  top_p         : 1
