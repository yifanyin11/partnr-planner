# Evaluation Generation Config

defaults:
  - ../../habitat_llm/conf/llm@eval_gen.llm: llama
  - hydra.output_subdir: null
  - _self_
eval_gen:
  llm:
    rlm_path: rlm/slurm/dev/server_list
    rlm_index: 0
    generation_params:
      do_sample: False
      engine: "{path to llama model}/CodeLlama-70b-Instruct-hf"  # load this model if inference_mode==hf
    inference_mode: hf # rlm
    max_tokens_proposition_call: 400
    max_tokens_dag_call: 75
    max_tokens_tie_call: 100
  generate: true                   # uses LLM to generate plaintext evaluation functions for a scene dataset
  pack: false                      # converts from plaintext, attaches dependencies, and packs the dataset ({run_name}.json.gz)
  merge: false                     # Merge scene-specific datasets into a single dataset.
  verify: false                    # Verifies episode/eval inferences in Hab-LLM. Trims the dataset to only successful episodes.
  path_to_dataset_in: data/datasets/dev/dataset_0/src  # must contain [scene_id]/dataset.json.gz, [scene_id]/scene_info.json
  scene_index: -1                  # indexes into scene_ids to generate evals for just that scene. -1 means all scenes.
  scene_ids: []                    # if empty, infer from path_to_dataset_in. Override using:
                                   #     eval_gen.scene_ids=["'106878945_174887058','108736872_177263607'"]
  output_path: data/datasets/dev/dataset_0
  run_name: gen
  proposition_prompt_file: dataset_generation/benchmark_generation/prompts_benchmark/eval_gen_prompts/propositions_prompt.txt
  os_proposition_prompt_file: dataset_generation/benchmark_generation/prompts_benchmark/eval_gen_prompts/os_propositions_prompt.txt
  use_spatial_temporal_correction_heuristic: true  # uses a heuristic correction to fix the temporal group of next-to relations.
  temporal_prompt_file: dataset_generation/benchmark_generation/prompts_benchmark/eval_gen_prompts/temporal_prompt.txt
  tie_prompt_file: dataset_generation/benchmark_generation/prompts_benchmark/eval_gen_prompts/tie_prompt.txt
  skip_temporal_prediction: False  # if True, assumes no temporal constraint.
  skip_tie_prediction: True        # If True, assumes no tied quantification constraints.
  skip_episode_default: True       # if True, the episode is marked "skip=True" in plaintext by default.
  is_from_templates: False         # Set to True if template episode examples should be injected into the prompt.
  filter_file_dir: data/hssd-hab/scene_filter_files  # path to receptacle filter files used to determine initial object placement type
  is_object_state_gen: False       # If False, ignores any object state predicate functions generated by the LLM.
  use_resolved_coreferences: True  # Applies when is_object_state_gen==True
                                   # If True, use the resolved_coref.json file within each scene directory.
                                   # Run coreference_resolution/resolve_src_dir.py to obtain resolved_coref.json.
  template_dataset: data/datasets/template_dataset.json.gz  # dataset of template episodes indexed by "template_task_number"
  template_dataset_dir: data/datasets/dev/2024_05_08_spatial_30k/300/300_annotated  # contains [scene_id]/metadata/episode_[i].json for the above template dataset
  predicate_vocabulary_file: dataset_generation/benchmark_generation/evaluation_generation/eval_predicates.json
  verification_num_proc: 1         # number of processes for parallelizing the verification step (1/CPU recommended)
  scene_metadata_dir: data/hssd-hab/metadata
